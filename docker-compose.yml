version: "3.1"
services:
  db:
    image: postgres:14.1-alpine
    restart: always
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: enterprise_analytics
    volumes:
     - ./db:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  data_loader:
    build: 
      context: ./db
      dockerfile: Dockerfile
    volumes:
      - ./data:/data
    depends_on:
      - db
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: enterprise_analytics
      POSTGRES_HOST: db

  airflow-db:
    image: postgres:14.1-alpine
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.com"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
    depends_on:
      airflow-db:
        condition: service_healthy

  webserver:
    build: 
      context: .
      dockerfile: Dockerfile
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-here
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/usr/local/airflow/dbt:rw
      - logs:/opt/airflow/logs
      - ./logs:/opt/airflow/logs:rw
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  scheduler:
    build: 
      context: .
      dockerfile: Dockerfile
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-here
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/usr/local/airflow/dbt:rw
      - logs:/opt/airflow/logs
      - ./logs:/opt/airflow/logs:rw
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

  triggerer:
    build: 
      context: .
      dockerfile: Dockerfile
    command: triggerer
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/usr/local/airflow/dbt:rw
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

volumes:
  logs: